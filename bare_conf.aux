\relax 
\bibstyle{IEEEtran}
\citation{dollar2014fast}
\citation{dollar2014fast}
\citation{sen2004robust}
\citation{sirikuntamat2015vehicle}
\citation{lu2014moving}
\citation{felzenszwalb2008discriminatively}
\citation{dollar2014fast}
\citation{dollar2010fastest}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Works}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Detection}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Tracking}{1}}
\citation{friedman2000additive}
\citation{dollar2014fast}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Outline of Aggregated Channel Features framework. First step: Computing several channels of given image $I$ with $\Omega $. Second step: Sum every blocks of channel features and smooth the resulting aggregated channels. Third step: Concatenation of feature channels in a vector. Fourth step: Training a boosted classifier \cite  {dollar2014fast}}}{2}}
\newlabel{ACFSteps}{{1}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Detection}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Aggregated Channel Features (ACF)}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Fast Feature Pyramids}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Tracking: Kalman Filter}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Kalman Model}{2}}
\citation{PMT}
\bibdata{ref}
\bibcite{sen2004robust}{1}
\bibcite{sirikuntamat2015vehicle}{2}
\bibcite{lu2014moving}{3}
\bibcite{felzenszwalb2008discriminatively}{4}
\bibcite{dollar2010fastest}{5}
\bibcite{dollar2014fast}{6}
\newlabel{tableResult}{{\unhbox \voidb@x \hbox {III-B}}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Comparison of our results with other teams in AUTCUP competition}}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Assignment of Detections and Tracks}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Implementation and Performance Evaluation}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-A}}Implementation}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-B}}Results}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{3}}
\@writefile{toc}{\contentsline {section}{References}{3}}
\bibcite{friedman2000additive}{7}
\bibcite{PMT}{8}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Result of our method in 12 videos of AUTCUP dataset}}{4}}
\newlabel{results}{{2}{4}}
