\relax 
\bibstyle{IEEEtran}
\citation{dollar2014fast}
\citation{dollar2014fast}
\citation{sen2004robust}
\citation{sirikuntamat2015vehicle}
\citation{lu2014moving}
\citation{felzenszwalb2008discriminatively}
\citation{li2014integrating}
\citation{huang2015densebox}
\citation{dollar2014fast}
\citation{dollar2014fast}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Works}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-A}}Detection}{1}}
\citation{zhang2013method}
\citation{kalal2012tracking}
\citation{dollar2014fast}
\citation{dollar2010fastest}
\citation{friedman2000additive}
\citation{dollar2014fast}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Outline of Aggregated Channel Features framework. First step: Computing several channels of given image $I$ with $\Omega $. Second step: Sum every blocks of channel features and smooth the resulting aggregated channels. Third step: Concatenation of feature channels in a vector. Fourth step: Training a boosted classifier \cite  {dollar2014fast}}}{2}}
\newlabel{ACFSteps}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {II-B}}Tracking}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {III}Detection}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Aggregated Channel Features (ACF)}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Fast Feature Pyramids}{2}}
\citation{PMT}
\newlabel{tableResult}{{\unhbox \voidb@x \hbox {III-B}}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Comparison of our results with other teams in AUTCUP competition}}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Tracking: Kalman Filter}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Kalman Model}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Assignment of Detections and Tracks}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Implementation and Performance Evaluation}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-A}}Implementation}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {V-B}}Results}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{3}}
\bibdata{ref}
\bibcite{sen2004robust}{1}
\bibcite{sirikuntamat2015vehicle}{2}
\bibcite{lu2014moving}{3}
\bibcite{felzenszwalb2008discriminatively}{4}
\bibcite{li2014integrating}{5}
\bibcite{huang2015densebox}{6}
\bibcite{dollar2014fast}{7}
\bibcite{zhang2013method}{8}
\bibcite{kalal2012tracking}{9}
\bibcite{dollar2010fastest}{10}
\bibcite{friedman2000additive}{11}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Result of our method in 12 videos of AUTCUP dataset}}{4}}
\newlabel{results}{{2}{4}}
\@writefile{toc}{\contentsline {section}{References}{4}}
